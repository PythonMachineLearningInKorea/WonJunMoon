XGBoost Part4 : XGBOOST OPTIMIZATIONS
(https://www.youtube.com/watch?v=oRrKeUCEbq8)

Large Dataset 에 좋게함.

GAIN 이 제일 큰 것을 XGBOOST 는 사용함. 나중 걱정안하고.
당장 최선의 선택을 하는 Greedy Algorithm 이다.(Greedy Algorithm 의 정의)
이렇게 하지 않는다면 root 에서 나누는 것은 밑에 branch 를 모두 나눠보기 전까지 지연 될 것이며, Greedy Algorithm 의 특징을 가짐으로써 relatively fast 하다는 특징을 가진다.

하지만 dataset 에 따라 greedy algorithm 도 한계를 가진다. 
이를 위하여, Approximate Greedy Algorithm 이 존재한다.
Observation 이 많다면 매 threshold 를 전부 보는 것이 아니라 quantile 만을 이용하여 
threshold 로 이용한다.
(part1, 2 를 보면 매 데이터 사이마다 GAIN 을 구하여 비교한다고 써있지만 approximate 는 딱 네개의 지점만을 이용)

하나의 quantile 만을 이용한다면 Similarity Score 같은 것을 구할 필요가 없으니 매우 빠를테지만 성능은 좋지 않을 것이다.
두개, 세개.... 더 많은 threshold를 이용하다면 성능이 증가할 것이다.
하지만 threshold 갯수가 증가한다면 계산량이 많아져 속도가 느려진다.

Default 값으로 Approximate Greedy Algorithm 은 33개 정도의 quantile 값을 이용한다.
정확히 왜 33개가 아니라 33개정도라고 표현하는가?

이를 위해 Parallel learning, weighted quantile sketch 를 알아야한다.
Dataset 이 무지크다면, 정리하고 quantile 을 찾는 것은 매우 오래걸릴 것이다.
Sketch Algorithm 은 이러한 approximate solution 을 찾는 것을 도와준다.

XGB는 데이터가 많아서 다른 컴퓨터들에 나눠 저장했다고 가정하였을때, Quantile Sketch Algorithm 을 이용하여 Approximate histogram을 만들고 이를 이용하여 Approximate quantile 을 구하면, 이를 Approximate Greedy Algorithm 이 사용하는 식으로 이용한다.
이는 그냥 Quantile Sketch Algorithm이지만, XGB는 weighted quantile sketch 를 이용한다.
Weights는 quantile 을 구할때 각각의 quantile 에 속하는 데이터의 수를 맞춰주기 위한 것이다.
이러한 weight 는 Cover 에서 derive 되게 되고, Hessian 즉 2nd deriva. 이니 Regression 에서는 weights 는 전부 1 이다. 

Classification 에서는 Weight = Previously Probability * ( 1- Previously Probability) 이다.
0.5 에 가까우면 갈수록 weight 는 커지고
끝쪽에 갈수록 weights 가 relatively small 하다. 
ex) 0.1 이라하면 0..1 * 0.9 = 0.09 이다.

0.5 이면 어떻게 분류 할지 잘모르는 것.. 0.6 이면 0.6 * 0.4 = 0.24 이다. 그래서 weight 이 크다.

하지만 positive 는 negative residual 을 없앨 수 있으니 predicted probability 를 improve 하기 힘들수있는 문제가 있다.
따라서 XGB 는 quantile 을 weight 을 비슷하게 만들어주려고 한다.

XGB는 Approximate Greedy Algorithm 을 쓰기에 Parallel Learning 을 지원한다. 이는 dataset 을 컴퓨터들에 나누어 동시에 
작업할수 있게 하는 것이다..

이런 이유로 Large Dataset 에서는 좋다.

반면에 Small Dataset 일 경우에 XGBoost 는 일반적인 그리디 알고리즘을 사용한다..

결측치가 있는 경우에도 Residual 을 Sparsity-Aware Split Finding 을 이용하여 계산 할 수 있다
데이터가 큰 순서대로 정렬할 수 없으니 데이터가 있는 것과 없는 것으로 테이블을 두개로 나눈다.
그리고는 데이터가 있는 것들을 기준으로 데이터를 나누어 GAIN 을 구한다. 이때 결측치의 잔차를 이용하여 
모든 브랜치에 넣어서 GAIN(left, right) 을 구한다..
이런 식으로 모든 브랜치를 나누는 기준에 맞춰 GAIN 을 구하면 된다.

XGBOOST 의 또 중요한 점이 있는데 Cache 에 관련 된 것이다.
Memory 는 Cache, MM, HDD 가 있는데 프로그램을 빨리 돌리려면 캐시메모리를 잘 활용해야한다.
따라서 XGB는 Gradient, Hessians 를 캐시에 저장함으로써 Similarity Score 과 Output Value 계산을 빠르게한다.

BlOCKS FOR OUT OF CORE COMPUTATION
데이터셋이 MM 에 넣기 너무 크면 HDD 에 저장해야한다. HDD 는 접근속도가 매우느리기에 이를 최소화하기 위해 압축을 한다.
압축하고 푸는데 시간이 들긴하지만, 하드드라이브에서 데이터를 읽는 것보다는 매우 빠르다.

또한 하나 이상의 하드드라이브가 사용가능하다면, XGB는 Sharding 이라는 데이터베이스 기법을 사용해서 disk access 속도를 높인다.
이는 각 하드에 중복되지 않게 데이터를 나누어 저장한 후, 데이터 사용시에 동시에 읽어 온다.

마지막으로 XGB는 random subset of features 만 보고도 어떻게 데이터를 나눌지 알수있어 속도를 빠르게 할 수 있다.



